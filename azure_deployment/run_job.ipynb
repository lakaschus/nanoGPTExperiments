{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code CXZESVAZX to authenticate.\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"id\": \"ea54fca2-3a3e-4b3b-91e7-a7bf971e0443\",\n",
      "    \"isDefault\": true,\n",
      "    \"name\": \"Azure subscription 1\",\n",
      "    \"state\": \"Enabled\",\n",
      "    \"tenantId\": \"1fdb6d07-480b-4c31-a913-e847d51e7446\",\n",
      "    \"user\": {\n",
      "      \"name\": \"Philliplakaschus@gmail.com\",\n",
      "      \"type\": \"user\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = !az account show --query id --output tsv\n",
    "subscription_id = subscription_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfigTree([('RESOURCE_GROUP', 'rg-llm-experiments'), ('WORKSPACE_NAME', 'yaol-llm-experiments'), ('REGION', 'eastus'), ('COMPUTE_VM', 'Standard_NC6s_v3'), ('COMPUTE_NAME', 'teslav100')])\n"
     ]
    }
   ],
   "source": [
    "# Get the resource group name and workspace name for the workspace from the config.conf file using pyhocon\n",
    "from pyhocon import ConfigFactory\n",
    "config = ConfigFactory.parse_file(\"config.conf\")\n",
    "resource_group = config.get_string(\"RESOURCE_GROUP\")\n",
    "workspace_name = config.get_string(\"WORKSPACE_NAME\")\n",
    "compute_name = config.get_string(\"COMPUTE_NAME\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7f920fba6bc0>,\n",
      "         subscription_id=ea54fca2-3a3e-4b3b-91e7-a7bf971e0443,\n",
      "         resource_group_name=rg-llm-experiments,\n",
      "         workspace_name=yaol-llm-experiments)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from azure.ai.ml import MLClient, Input, Output\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# NOTE: It's very import to always set the resource_group and workspace_name when creating the MLClient\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id=subscription_id, resource_group_name=resource_group, workspace_name=workspace_name\n",
    ")\n",
    "print(ml_client)\n",
    "\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "ws = Workspace(subscription_id=subscription_id, resource_group=resource_group, workspace_name=workspace_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Params / Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../configs/azure_configs_debugging.json\") as f:\n",
    "    params = json.load(f)\n",
    "    train_params = params['train_params']\n",
    "    sample_params = params['sample_params']\n",
    "    dataset_params = params['dataset_params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': 'mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04', 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'nano_gpt_env', 'description': 'Custom environment for nano gpt training', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': False, 'id': '/subscriptions/ea54fca2-3a3e-4b3b-91e7-a7bf971e0443/resourceGroups/rg-llm-experiments/providers/Microsoft.MachineLearningServices/workspaces/yaol-llm-experiments/environments/nano_gpt_env/versions/8', 'Resource__source_path': '', 'base_path': '/home/lakaschus/python/nanoGPTExperiments/azure_deployment', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f92699255d0>, 'serialize': <msrest.serialization.Serializer object at 0x7f920ed22fe0>, 'version': '8', 'conda_file': {'channels': ['nvidia/label/cuda-11.8.0', 'pytorch', 'conda-forge', 'defaults'], 'dependencies': ['python=3.10', 'pip', {'pip': ['numpy', 'transformers', 'datasets', 'tiktoken', 'mlflow', 'tqdm', 'azure-ai-ml', 'pyhocon', 'azureml-mlflow', 'azureml-fsspec']}, 'pytorch', 'torchvision', 'torchaudio', 'cuda', 'pytorch-cuda=11.8'], 'name': 'nano_gpt'}, 'build': None, 'inference_config': None, 'os_type': 'Linux', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': '{\\n  \"channels\": [\\n    \"nvidia/label/cuda-11.8.0\",\\n    \"pytorch\",\\n    \"conda-forge\",\\n    \"defaults\"\\n  ],\\n  \"dependencies\": [\\n    \"python=3.10\",\\n    \"pip\",\\n    {\\n      \"pip\": [\\n        \"numpy\",\\n        \"transformers\",\\n        \"datasets\",\\n        \"tiktoken\",\\n        \"mlflow\",\\n        \"tqdm\",\\n        \"azure-ai-ml\",\\n        \"pyhocon\",\\n        \"azureml-mlflow\",\\n        \"azureml-fsspec\"\\n      ]\\n    },\\n    \"pytorch\",\\n    \"torchvision\",\\n    \"torchaudio\",\\n    \"cuda\",\\n    \"pytorch-cuda=11.8\"\\n  ],\\n  \"name\": \"nano_gpt\"\\n}'})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env = Environment(\n",
    "    name='nano_gpt_env',\n",
    "    description=\"Custom environment for nano gpt training\",\n",
    "    conda_file=\"../environment.yml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04\"\n",
    ")\n",
    "ml_client.environments.create_or_update(custom_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# from azure.ai.ml.entities import Data\n",
    "# import os\n",
    "\n",
    "# def build_cmd(script, param_dict):\n",
    "#     return f\"python {script}\" + \"\".join(f\" --{k}\" if v is True else f\" --{k}={v}\" for k, v in param_dict.items())\n",
    "\n",
    "# def get_or_create_dataset(dataset_name):\n",
    "#     try:\n",
    "#         # Try to get the existing dataset\n",
    "#         dataset_train = ml_client.data.get(name=dataset_name + '_train', version=\"latest\")\n",
    "#         dataset_val = ml_client.data.get(name=dataset_name + '_val', version=\"latest\")\n",
    "#         print(f\"Dataset {dataset_name} already exists. Using existing version.\")\n",
    "#         return dataset_train, dataset_val\n",
    "#     except Exception:\n",
    "#         print(f\"Dataset {dataset_name} doesn't exist or not registered.\")\n",
    "        \n",
    "#         dataset_path = './data/' + dataset_name\n",
    "        \n",
    "#         if os.path.exists(dataset_path):\n",
    "#             print(f\"Dataset {dataset_name} already exists at {dataset_path}.\")\n",
    "#         else:\n",
    "#             print(f\"Dataset {dataset_name} doesn't exist yet locally, create it...\")\n",
    "#             cmd = build_cmd('../data/prepare.py', dataset_params)\n",
    "#             subprocess.run(cmd, shell=True)\n",
    "        \n",
    "#             # Create the dataset\n",
    "#         dataset_train = Data(\n",
    "#             name=dataset_name + '_train',\n",
    "#             description=f\"train dataset for {dataset_name}\",\n",
    "#             path=dataset_path + '/train.bin',\n",
    "#             type=\"uri_file\",\n",
    "#             version=\"latest\"\n",
    "#         )\n",
    "        \n",
    "#         dataset_val = Data(\n",
    "#             name=dataset_name + '_val',\n",
    "#             description=f\"val dataset for {dataset_name}\",\n",
    "#             path=dataset_path + '/val.bin',\n",
    "#             type=\"uri_file\",\n",
    "#             version=\"latest\"\n",
    "#         )\n",
    "\n",
    "#         ml_client.data.create_or_update(dataset_train)\n",
    "#         ml_client.data.create_or_update(dataset_val)\n",
    "#         print(f\"Dataset {dataset_name} has been registered/updated.\")\n",
    "#         return dataset_train, dataset_val\n",
    "\n",
    "# dataset_train, dataset_val = get_or_create_dataset(dataset_params['dataset_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset openwebtext-10k already exists. Using existing version.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from azure.ai.ml.entities import Data\n",
    "\n",
    "def build_cmd(script, param_dict):\n",
    "    return f\"python {script}\" + \"\".join(f\" --{k}\" if v is True else f\" --{k}={v}\" for k, v in param_dict.items())\n",
    "\n",
    "def create_dataset(dataset_name):\n",
    "    try:\n",
    "        # Try to get the existing dataset\n",
    "        dataset = ml_client.data.get(name=dataset_name, version=\"latest\")\n",
    "        print(f\"Dataset {dataset_name} already exists. Using existing version.\")\n",
    "    except Exception:\n",
    "        print(f\"Dataset {dataset_name} doesn't exist. Creating new dataset.\")\n",
    "        cmd = build_cmd('../data/prepare.py', dataset_params)\n",
    "        subprocess.run(cmd, shell=True)\n",
    "        dataset_path = '../data/' + dataset_name.split(\"/\")[-1].replace(\"-\", \"_\")\n",
    "            # Create the dataset\n",
    "        dataset = Data(\n",
    "            name=dataset_name,\n",
    "            description=f\"Dataset for {dataset_name}\",\n",
    "            path=dataset_path,\n",
    "            type=\"uri_folder\",\n",
    "            version=\"latest\"\n",
    "        )\n",
    "\n",
    "        ml_client.data.create_or_update(dataset)\n",
    "        print(f\"Dataset {dataset_name} has been registered/updated.\")\n",
    "\n",
    "create_dataset(dataset_params['dataset_id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import UserIdentityConfiguration\n",
    "\n",
    "train_cmd = build_cmd('train.py', train_params)\n",
    "\n",
    "# configure job\n",
    "train_job = command(\n",
    "    code=\"../src\",\n",
    "    command=train_cmd,\n",
    "    environment=custom_env,\n",
    "    compute=compute_name,\n",
    "    display_name=train_params[\"experiment_name\"],\n",
    "    experiment_name=train_params[\"experiment_name\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Warning: the provided asset name 'nano_gpt_env' will not be used for anonymous registration\n",
      "Warning: the provided asset name 'nano_gpt_env' will not be used for anonymous registration\n",
      "Uploading src (0.05 MBs): 100%|██████████| 54860/54860 [00:01<00:00, 27526.23it/s]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitor your job at https://ml.azure.com/runs/good_tray_3zky1f1w94?wsid=/subscriptions/ea54fca2-3a3e-4b3b-91e7-a7bf971e0443/resourcegroups/rg-llm-experiments/workspaces/yaol-llm-experiments&tid=1fdb6d07-480b-4c31-a913-e847d51e7446\n"
     ]
    }
   ],
   "source": [
    "returned_job = ml_client.create_or_update(train_job)\n",
    "aml_url = returned_job.studio_url\n",
    "print(\"Monitor your job at\", aml_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_job = command(\n",
    "    code=\"../src\",\n",
    "    command=\"python train.py\",\n",
    "    inputs={\n",
    "        f\"{dataset_name} dataset\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"model\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    }\n",
    "    environment=custom_env,\n",
    "    compute=compute_name,\n",
    "    display_name=train_params[\"experiment_name\"],\n",
    "    experiment_name=train_params[\"experiment_name\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
