{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code CXZESVAZX to authenticate.\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"id\": \"ea54fca2-3a3e-4b3b-91e7-a7bf971e0443\",\n",
      "    \"isDefault\": true,\n",
      "    \"name\": \"Azure subscription 1\",\n",
      "    \"state\": \"Enabled\",\n",
      "    \"tenantId\": \"1fdb6d07-480b-4c31-a913-e847d51e7446\",\n",
      "    \"user\": {\n",
      "      \"name\": \"Philliplakaschus@gmail.com\",\n",
      "      \"type\": \"user\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = !az account show --query id --output tsv\n",
    "subscription_id = subscription_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the resource group name and workspace name for the workspace from the config.conf file using pyhocon\n",
    "from pyhocon import ConfigFactory\n",
    "config = ConfigFactory.parse_file(\"config.conf\")\n",
    "resource_group = config.get_string(\"RESOURCE_GROUP\")\n",
    "workspace_name = config.get_string(\"WORKSPACE_NAME\")\n",
    "compute_name = config.get_string(\"COMPUTE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7f994c3c6cb0>,\n",
      "         subscription_id=ea54fca2-3a3e-4b3b-91e7-a7bf971e0443,\n",
      "         resource_group_name=rg-llm-experiments,\n",
      "         workspace_name=yaol-llm-experiments)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from azure.ai.ml import MLClient, Input, Output\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# NOTE: It's very import to always set the resource_group and workspace_name when creating the MLClient\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace_name\n",
    ")\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': 'mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04', 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'nano_gpt_env', 'description': 'Custom environment for nano gpt training', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': False, 'id': '/subscriptions/ea54fca2-3a3e-4b3b-91e7-a7bf971e0443/resourceGroups/rg-llm-experiments/providers/Microsoft.MachineLearningServices/workspaces/yaol-llm-experiments/environments/nano_gpt_env/versions/3', 'Resource__source_path': '', 'base_path': '/home/lakaschus/python/nanoGPTExperiments/azure_deployment', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f995c20c880>, 'serialize': <msrest.serialization.Serializer object at 0x7f995db06410>, 'version': '3', 'conda_file': {'channels': ['nvidia/label/cuda-11.8.0', 'pytorch', 'conda-forge', 'defaults'], 'dependencies': ['python=3.10', 'pip', {'pip': ['numpy', 'transformers', 'datasets', 'tiktoken', 'mlflow', 'tqdm', 'azure-ai-ml', 'pyhocon', 'azureml-mlflow']}, 'pytorch', 'torchvision', 'torchaudio', 'cuda', 'pytorch-cuda=11.8'], 'name': 'nano_gpt'}, 'build': None, 'inference_config': None, 'os_type': 'Linux', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': '{\\n  \"channels\": [\\n    \"nvidia/label/cuda-11.8.0\",\\n    \"pytorch\",\\n    \"conda-forge\",\\n    \"defaults\"\\n  ],\\n  \"dependencies\": [\\n    \"python=3.10\",\\n    \"pip\",\\n    {\\n      \"pip\": [\\n        \"numpy\",\\n        \"transformers\",\\n        \"datasets\",\\n        \"tiktoken\",\\n        \"mlflow\",\\n        \"tqdm\",\\n        \"azure-ai-ml\",\\n        \"pyhocon\",\\n        \"azureml-mlflow\"\\n      ]\\n    },\\n    \"pytorch\",\\n    \"torchvision\",\\n    \"torchaudio\",\\n    \"cuda\",\\n    \"pytorch-cuda=11.8\"\\n  ],\\n  \"name\": \"nano_gpt\"\\n}'})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment, Data\n",
    "\n",
    "custom_env = Environment(\n",
    "    name='nano_gpt_env',\n",
    "    description=\"Custom environment for nano gpt training\",\n",
    "    conda_file=\"../environment.yml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04\"\n",
    ")\n",
    "ml_client.environments.create_or_update(custom_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Params / Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../configs/azure_configs_debugging.json\") as f:\n",
    "    params = json.load(f)\n",
    "    train_params = params['train_params']\n",
    "    sample_params = params['sample_params']\n",
    "    dataset_params = params['dataset_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cmd(script, param_dict):\n",
    "    return f\"python {script}.py\" + \"\".join(f\" --{k}={v}\" for k, v in param_dict.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_dataset(\n",
    ") -> Data:\n",
    "    try:\n",
    "        # Try to get the existing dataset\n",
    "        dataset = ml_client.data.get(name=dataset_name, version=\"latest\")\n",
    "        print(f\"Dataset {dataset_name} already exists. Using existing version.\")\n",
    "        return dataset\n",
    "    except Exception:\n",
    "        print(f\"Dataset {dataset_name} doesn't exist. Creating new dataset.\")\n",
    "\n",
    "    build_cmd('data/prepare.py', dataset_params)\n",
    "\n",
    "        # Create the dataset\n",
    "    dataset = Data(\n",
    "        name=dataset_name,\n",
    "        description=f\"Dataset for {dataset_name}\",\n",
    "        path=dataset_path,\n",
    "        type=\"uri_file\"\n",
    "    )\n",
    "    \n",
    "    ml_client.data.create_or_update(dataset)\n",
    "    print(f\"Dataset {dataset_name} has been registered/updated.\")\n",
    "    return dataset\n",
    "\n",
    "# TODO CONTINUE HER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_dataset(\n",
    "    dataset_name: str,\n",
    "    dataset_id: str,\n",
    "    num_proc: int,\n",
    "    test_size: float,\n",
    "    seed: int,\n",
    "    encoding: str\n",
    ") -> Data:\n",
    "    try:\n",
    "        # Try to get the existing dataset\n",
    "        dataset = ml_client.data.get(name=dataset_name, version=\"latest\")\n",
    "        print(f\"Dataset {dataset_name} already exists. Using existing version.\")\n",
    "        return dataset\n",
    "    except Exception:\n",
    "        print(f\"Dataset {dataset_name} doesn't exist. Creating new dataset.\")\n",
    "        \n",
    "        # Define dataset creation component\n",
    "        @command(\n",
    "            name=\"create_dataset\",\n",
    "            display_name=\"Create Dataset\",\n",
    "            description=\"Create and preprocess the dataset\",\n",
    "            environment=custom_env,\n",
    "        )\n",
    "        def create_dataset(\n",
    "            dataset_id: str,\n",
    "            num_proc: int,\n",
    "            test_size: float,\n",
    "            seed: int,\n",
    "            encoding: str\n",
    "        ) -> Output(type=\"uri_folder\"):\n",
    "            import subprocess\n",
    "            \n",
    "            script_path = \"create_dataset.py\"\n",
    "            cmd = [\n",
    "                \"python\", script_path,\n",
    "                \"--dataset_id\", dataset_id,\n",
    "                \"--num_proc\", str(num_proc),\n",
    "                \"--test_size\", str(test_size),\n",
    "                \"--seed\", str(seed),\n",
    "                \"--encoding\", encoding\n",
    "            ]\n",
    "            \n",
    "            subprocess.run(cmd, check=True)\n",
    "            \n",
    "            return Output(path=\"./\")\n",
    "\n",
    "        # Create the dataset\n",
    "        job = create_dataset(\n",
    "            dataset_id=dataset_id,\n",
    "            num_proc=num_proc,\n",
    "            test_size=test_size,\n",
    "            seed=seed,\n",
    "            encoding=encoding\n",
    "        )\n",
    "        \n",
    "        returned_job = ml_client.jobs.create_or_update(job)\n",
    "        returned_job = ml_client.jobs.stream(returned_job.name)\n",
    "\n",
    "        # Register the dataset as a data asset\n",
    "        dataset = Data(\n",
    "            name=dataset_name,\n",
    "            description=f\"Processed dataset from {dataset_id}\",\n",
    "            path=returned_job.outputs.output.path,\n",
    "            type=\"uri_folder\"\n",
    "        )\n",
    "        ml_client.data.create_or_update(dataset)\n",
    "        \n",
    "        print(f\"Dataset {dataset_name} has been created and registered.\")\n",
    "        return dataset\n",
    "\n",
    "# Get or create the dataset\n",
    "dataset_name = f\"{train_params['dataset']}_processed\"\n",
    "dataset = get_or_create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    dataset_id=train_params.get(\"dataset_id\", \"stas/openwebtext-10k\"),\n",
    "    num_proc=train_params.get(\"num_proc\", 1),\n",
    "    test_size=train_params.get(\"test_size\", 0.1),\n",
    "    seed=train_params.get(\"seed\", 2351),\n",
    "    encoding=train_params.get(\"encoding\", \"gpt2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the job, remove all unnecessary files from the src folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert the below command into a pipeline\n",
    "# Step 1: Create and register a dataset given on the dataset name; Only update the dataset if it's not already registered or changed\n",
    "# Step 2: Load and preview the dataset\n",
    "# Step 3: Run the training script with the given parameters\n",
    "# Step 4: Create a sample from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationException",
     "evalue": "No such file or directory: train.yml",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/azure/ai/ml/_utils/utils.py:313\u001b[0m, in \u001b[0;36mload_yaml\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 313\u001b[0m     cm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDefaultOpenEncoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.yml'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_component\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;129m@load_component\u001b[39m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain.yml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[1;32m      6\u001b[0m     out_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      7\u001b[0m     dataset: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      8\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m      9\u001b[0m     learning_rate: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m     10\u001b[0m     max_iters: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     11\u001b[0m     experiment_name: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m     12\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muri_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@load_component\u001b[39m(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_sample\u001b[39m(\n\u001b[1;32m     17\u001b[0m     out_dir: Input(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muri_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     18\u001b[0m     num_samples: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     19\u001b[0m     max_new_tokens: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m     20\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muri_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/azure/ai/ml/entities/_load_functions.py:501\u001b[0m, in \u001b[0;36mload_component\u001b[0;34m(source, relative_origin, params_override, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m version \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source:\n\u001b[0;32m--> 501\u001b[0m     component_entity \u001b[38;5;241m=\u001b[39m \u001b[43mload_common\u001b[49m\u001b[43m(\u001b[49m\u001b[43mComponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_origin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_override\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m client \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m version:\n\u001b[1;32m    503\u001b[0m     component_entity \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcomponents\u001b[38;5;241m.\u001b[39mget(name, version)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/azure/ai/ml/entities/_load_functions.py:99\u001b[0m, in \u001b[0;36mload_common\u001b[0;34m(cls, source, relative_origin, params_override, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             relative_origin \u001b[38;5;241m=\u001b[39m _DEFAULT_RELATIVE_ORIGIN\n\u001b[1;32m     98\u001b[0m params_override \u001b[38;5;241m=\u001b[39m params_override \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m---> 99\u001b[0m yaml_dict \u001b[38;5;241m=\u001b[39m \u001b[43m_try_load_yaml_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mcls\u001b[39m, type_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_cls_and_type(data\u001b[38;5;241m=\u001b[39myaml_dict, params_override\u001b[38;5;241m=\u001b[39mparams_override)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/azure/ai/ml/entities/_load_functions.py:135\u001b[0m, in \u001b[0;36m_try_load_yaml_dict\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_load_yaml_dict\u001b[39m(source: Union[\u001b[38;5;28mstr\u001b[39m, PathLike, IO[AnyStr]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 135\u001b[0m     yaml_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m yaml_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# This happens when a YAML is empty.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget yaml file is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.10/site-packages/azure/ai/ml/_utils/utils.py:316\u001b[0m, in \u001b[0;36mload_yaml\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    315\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 316\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ValidationException(\n\u001b[1;32m    317\u001b[0m             message\u001b[38;5;241m=\u001b[39mmsg\u001b[38;5;241m.\u001b[39mformat(source),\n\u001b[1;32m    318\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39mmsg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[file_path]\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    319\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[1;32m    320\u001b[0m             target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mGENERAL,\n\u001b[1;32m    321\u001b[0m             error_type\u001b[38;5;241m=\u001b[39mValidationErrorType\u001b[38;5;241m.\u001b[39mFILE_OR_FOLDER_NOT_FOUND,\n\u001b[1;32m    322\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;66;03m# source is a subclass of IO\u001b[39;00m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m source\u001b[38;5;241m.\u001b[39mreadable():\n",
      "\u001b[0;31mValidationException\u001b[0m: No such file or directory: train.yml"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "@load_component(source=\"train.yml\")\n",
    "def train_model(\n",
    "    out_dir: str,\n",
    "    dataset: str,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    max_iters: int,\n",
    "    experiment_name: str\n",
    ") -> Output(type=\"uri_folder\"):\n",
    "    pass\n",
    "\n",
    "@load_component(source=\"sample.yml\")\n",
    "def generate_sample(\n",
    "    out_dir: Input(type=\"uri_folder\"),\n",
    "    num_samples: int,\n",
    "    max_new_tokens: int\n",
    ") -> Output(type=\"uri_folder\"):\n",
    "    pass\n",
    "\n",
    "# Define the pipeline\n",
    "@pipeline(name=\"nano-gpt-pipeline\", description=\"Pipeline for training and sampling from a nano GPT model\")\n",
    "def nano_gpt_pipeline(\n",
    "    out_dir: str,\n",
    "    dataset: str,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    max_iters: int,\n",
    "    experiment_name: str,\n",
    "    num_samples: int,\n",
    "    max_new_tokens: int\n",
    "):\n",
    "    train_job = train_model(\n",
    "        out_dir=out_dir,\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        max_iters=max_iters,\n",
    "        experiment_name=experiment_name\n",
    "    )\n",
    "    \n",
    "    sample_job = generate_sample(\n",
    "        out_dir=train_job.outputs.model_output,\n",
    "        num_samples=num_samples,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    \n",
    "    return {\"train_output\": train_job.outputs.model_output, \"sample_output\": sample_job.outputs.samples}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the pipeline job\n",
    "pipeline_job = nano_gpt_pipeline(\n",
    "    out_dir=train_params[\"out_dir\"],\n",
    "    dataset=train_params[\"dataset\"],\n",
    "    batch_size=train_params[\"batch_size\"],\n",
    "    learning_rate=train_params[\"learning_rate\"],\n",
    "    max_iters=train_params[\"max_iters\"],\n",
    "    experiment_name=train_params[\"experiment_name\"],\n",
    "    num_samples=sample_params[\"num_samples\"],\n",
    "    max_new_tokens=sample_params[\"max_new_tokens\"]\n",
    ")\n",
    "\n",
    "# Submit the job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=train_params[\"experiment_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: the provided asset name 'nano_gpt_env' will not be used for anonymous registration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: the provided asset name 'nano_gpt_env' will not be used for anonymous registration\n",
      "Uploading src (27.77 MBs): 100%|##########| 27773706/27773706 [00:08<00:00, 3117742.11it/s]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitor your job at https://ml.azure.com/runs/calm_muscle_lwb12zg6gc?wsid=/subscriptions/ea54fca2-3a3e-4b3b-91e7-a7bf971e0443/resourcegroups/rg-nano-gpt/workspaces/ml-nano-gpt&tid=1fdb6d07-480b-4c31-a913-e847d51e7446\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command\n",
    "\n",
    "# configure job\n",
    "job = command(\n",
    "    code=\"../src\",\n",
    "    command=\"python pipeline.py\",\n",
    "    environment=custom_env,\n",
    "    compute=compute_name,\n",
    "    display_name=params[\"experiment_name\"],\n",
    "    experiment_name=params[\"experiment_name\"],\n",
    ")\n",
    "\n",
    "# submit job\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "aml_url = returned_job.studio_url\n",
    "print(\"Monitor your job at\", aml_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>nano-gpt-training</td><td>heroic_spinach_qslcj4s1d6</td><td>command</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/heroic_spinach_qslcj4s1d6?wsid=/subscriptions/ea54fca2-3a3e-4b3b-91e7-a7bf971e0443/resourcegroups/rg-nano-gpt/workspaces/ml-nano-gpt&amp;tid=1fdb6d07-480b-4c31-a913-e847d51e7446\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "Command({'parameters': {}, 'init': False, 'name': 'heroic_spinach_qslcj4s1d6', 'type': 'command', 'status': 'Starting', 'log_files': None, 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/lakaschus/nanoGPTExperiments.git', 'mlflow.source.git.branch': 'azure_deployment', 'mlflow.source.git.commit': 'bb2b7cdcb9c88ee0193197c31e1bd011db84a241', 'azureml.git.dirty': 'True', '_azureml.ComputeTargetType': 'amlctrain', 'ContentSnapshotId': '73a457c3-74ab-43c8-ac66-cc81d19b4a6d'}, 'print_as_yaml': True, 'id': '/subscriptions/ea54fca2-3a3e-4b3b-91e7-a7bf971e0443/resourceGroups/rg-nano-gpt/providers/Microsoft.MachineLearningServices/workspaces/ml-nano-gpt/jobs/heroic_spinach_qslcj4s1d6', 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\d92343\\\\Documents\\\\IT\\\\Projects\\\\Private\\\\nanoGPTExperiments\\\\azure_deployment', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x000001DE4EE19AE0>, 'serialize': <msrest.serialization.Serializer object at 0x000001DE4EE19A50>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'nano-gpt-training', 'experiment_name': 'nano-gpt-training', 'compute': 'aml-cluster-nano-gpt', 'services': {'Tracking': {'endpoint': 'azureml://westus.api.azureml.ms/mlflow/v1.0/subscriptions/ea54fca2-3a3e-4b3b-91e7-a7bf971e0443/resourceGroups/rg-nano-gpt/providers/Microsoft.MachineLearningServices/workspaces/ml-nano-gpt?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/heroic_spinach_qslcj4s1d6?wsid=/subscriptions/ea54fca2-3a3e-4b3b-91e7-a7bf971e0443/resourcegroups/rg-nano-gpt/workspaces/ml-nano-gpt&tid=1fdb6d07-480b-4c31-a913-e847d51e7446', 'type': 'Studio'}}, 'comment': None, 'job_inputs': {}, 'job_outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.heroic_spinach_qslcj4s1d6', 'mode': 'rw_mount'}}, 'inputs': {}, 'outputs': {'default': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x000001DE4EE1A710>}, 'component': CommandComponent({'intellectual_property': None, 'auto_increment_version': True, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': False, 'auto_delete_setting': None, 'name': 'heroic_spinach_qslcj4s1d6', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\d92343\\\\Documents\\\\IT\\\\Projects\\\\Private\\\\nanoGPTExperiments\\\\azure_deployment', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x000001DE4EE19AE0>, 'serialize': <msrest.serialization.Serializer object at 0x000001DE4EE19C30>, 'command': 'python train.py --batch_size=32 --compile=False --device=cuda', 'code': '/subscriptions/ea54fca2-3a3e-4b3b-91e7-a7bf971e0443/resourceGroups/rg-nano-gpt/providers/Microsoft.MachineLearningServices/workspaces/ml-nano-gpt/codes/7587c5ac-670e-476b-a066-4cb10b468274/versions/1', 'environment_variables': {}, 'environment': '/subscriptions/ea54fca2-3a3e-4b3b-91e7-a7bf971e0443/resourceGroups/rg-nano-gpt/providers/Microsoft.MachineLearningServices/workspaces/ml-nano-gpt/environments/nano_gpt_env/versions/2', 'distribution': None, 'resources': None, 'queue_settings': None, 'version': None, 'latest_version': None, 'schema': None, 'type': 'command', 'display_name': 'nano-gpt-training', 'is_deterministic': True, 'inputs': {}, 'outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.heroic_spinach_qslcj4s1d6', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {'status': 'Starting', 'parameters': {}}, 'additional_includes': []}), 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': {'Tracking': {'endpoint': 'azureml://westus.api.azureml.ms/mlflow/v1.0/subscriptions/ea54fca2-3a3e-4b3b-91e7-a7bf971e0443/resourceGroups/rg-nano-gpt/providers/Microsoft.MachineLearningServices/workspaces/ml-nano-gpt?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/heroic_spinach_qslcj4s1d6?wsid=/subscriptions/ea54fca2-3a3e-4b3b-91e7-a7bf971e0443/resourcegroups/rg-nano-gpt/workspaces/ml-nano-gpt&tid=1fdb6d07-480b-4c31-a913-e847d51e7446', 'type': 'Studio'}}, 'status': 'Starting', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x000001DE4EE19AE0>}, 'instance_id': '58f8c795-87db-481d-987f-49d72d226a32', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': 'nano_gpt_env:2', 'resources': {'instance_count': 1, 'shm_size': '2g'}, 'queue_settings': None, 'swept': False})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returned_job"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
